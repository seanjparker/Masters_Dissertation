\section{Model-free Agent}

In section \ref{sec:design:subsec:embed} we described the process for translating the computation graph, built in a machine learning framework, into an internal message passing graph neural network that can produce a latent space embedding, $z_t$, of the graph state $s_t$ at a time $t$. In our work, we used the PPO algorithm  described by Schulman et al. \cite{schulman2017proximal} as it brings three advantages, it was deliberately designed to be sample efficient, easy to implement, and stable to a wide range of values in hyperparameter selection. Its predecessors, such as TRPO \cite{schulman2017trust}, required off-policy learning using replay memory, which is often challenging to implement efficiently - especially with systems environments where rollouts are expensive to collect and store. Moreover, PPO is an on-policy algorithm that is compatible with stochastic gradient descent (SDG), meaning we can use it in combination with our graph network to train using hierarchical reinforcement learning. Algorithm \ref{algo:ppo} shows a variant of the PPO algorithm using a clipped objective, resulting in a simpler implementation compared to KL-penalty objective.

% We chose to use hierarchical reinforcement learning (HRL) to structure the learning of the model-free agent by breaking down the action predictions into sub-policies. As hypothesised by Nachum et al. \cite{nachum2019does}, HRL provides a unique benefit of improved environment exploration by temporally extended exploration as high-level actions correspond to multiple environment steps, thus, the HRL agent can explore more efficiently.

\input{sections/4rlopt/ppo.tex}

We use short online rollouts to collect a mini-batch of observations where a single trajectory begins with the unmodified graph and we iteratively apply transformations until we reach a terminal action or no further transformations can be applied. After each rollout we estimate the runtime which is used to calculate the reward for the rollout - we described the reward calculation in section \ref{sec:prob:subsec:rwd}.

After collection of $n$ rollouts, we train the agent using the data produced during each action step which is used to update the weights of the policy and value neural networks according to the PPO algorithm. One should note that as we require two actions to be selected (\texttt{xfer\textunderscore id} and \texttt{location\textunderscore id}), it requires two sets of results to be collected during the rollout, one for each action performed. Additionally, as we perform two actions, it doubles our overhead during training as we both store and perform backpropagation for four neural networks, the policy and value networks for each action. However, as we discussed in \ref{sec:prob:subsec:sap}, the alternative approach we considered would lead to lower agent performance during training due to the larger action space.
\section{Graph-level optimisation}

Performing optimisations at a higher, graph-level means that the resulting graph is---in terms of execution methodology---no different than the original graph prior to optimisation. Therefore, by performing graph-level optimisation we generate a platform and backend independent graph representation which can be further optimised by specialised software for custom hardware accelerators such as GPUs and TPUs.

Next, we define that two computation graphs, $\mathcal{G}$ and $\mathcal{G}'$ are semantically equivalent when $\forall \mathcal{I} : \mathcal{G}(\mathcal{I}) = \mathcal{G}'(\mathcal{I})$ where $\mathcal{I}$ is an arbitrary input tensor. We aim to find the optimal graph $\mathcal{G}^*$ that minimises the cost function, \texttt{cost}$(\mathcal{G})$, by performing a series of transformations to the computation graph - at each step, the specific transformation applied does not need to be strictly optimal. In fact, by applying optimisations that reduce graph runtime we further increase the state space for the search; a large state space is preferable in the reinforcement learning domain.

An important problem in graph-level optimisation is that of defining a set of varied, applicable transformations that can be used to optimise the graphs. As previously noted, prior work such as TensorFlow use a manually defined set of transformations and optimise greedily. On the other hand, TASO uses a fully automatic method to generate candidate transformations by performing a hash-based enumeration over all possible DNN operators that result in a semantically equivalent computation graph.

\begin{figure}[htbp]
  % preliminary
  \sbox\twosubbox{%
    \resizebox{\dimexpr.9\textwidth-1em}{!}{%
      \includegraphics[height=3cm]{sections/3problem/images/rewrite1}%
      \includegraphics[height=3cm]{sections/3problem/images/rewrite2}%
    }%
  }
  \setlength{\twosubht}{\ht\twosubbox}
  
  % typeset
  \centering
  \subcaptionbox{Tensor renaming substitution \label{fig:problem:rewrite-graph1}}{
    \includegraphics[height=\twosubht]{sections/3problem/images/rewrite1}
  }\quad
  \subcaptionbox{Common subgraph substitution \label{fig:problem:rewrite-graph2}}{
    \includegraphics[height=\twosubht]{sections/3problem/images/rewrite2}
  }
  \caption[Two examples of trivial graph substitutions]{Two examples of trivial graph substitutions that does not impact the overall runtime of the computation graph. The left sub-figure shows a simple renaming of the tensor inputs. The figure on the right shows that we have a common sub-graph between the source and the target graphs. In both cases we eliminate the duplicates as the hash of the two graphs will be identical.}
\end{figure}

In this work, we take the same approach as that of TASO and automatically generate the candidate graphs. We perform this as an offline step as it requires a large amount of computation to both generate and verify the candidate substitution; to place an upper bound on the computation, we limit the input tensor size to a maximum of 4x4x4x4 during the verification process. Following the generation and verification steps, we prune the collection to remove substitutions that are considered trivial and as such would not impact runtime. For example, trivial substitutions include input tensor renaming and common subgraphs, we show both techniques diagrammatically in Figure \ref{fig:problem:rewrite-graph1} and \ref{fig:problem:rewrite-graph2} respectively.

\input{sections/4rlopt/gnn.tex}

\section{Reinforcement Learning formulation}
In the following section we will describe how to represent the computation graph optimisation problem in the reinforcement learning domain by describing the key components of the system. We describe the system environment in which the agents act, the state-action space, and finally the reward functions for both the model-free and model-based agents which we used to determine the optimal reward signal to train the agents.

\subsection{System environment}
\label{sec:prob:subsec:sysenv}

In order to train a reinforcement learning agent, it necessary that we have access to an environment that, given the current environment state, the agent can take an action. After taking the chosen action, the environment is updated into a new state and the agent receives a reward signal. Typically, one uses a mature environment such as OpenAI Gym \cite{brockman2016openai} or OpenSpiel \cite{LanctotEtAl2019OpenSpiel} as the quality of the environment often has a significant effect on the stability of training. Moreover, using an environment that uses a common interface allows researchers to implement algorithms with ease and, importantly, reproduce results from published conference papers.

In our work, we implemented an environment that follows the OpenAI Gym API standard stepping an environment, that is, we have a function \texttt{step(action)} that accepts a single parameter, the action requested by the agent to be performed in the environment. The \texttt{step} function returns a 4-tuple \texttt{(next\textunderscore state, reward, terminal, extra\textunderscore info)}. \texttt{extra\textunderscore info} is a dictionary which can store arbitrary data. The environment in our project has a structure that is shown diagrammatically in figure \ref{fig:problem:sys-env}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.75\columnwidth]{sections/3problem/images/sysenv}
  \caption[RL system environment]{Data flow between components of the RL system. Although this diagram shows the setup for both the training of the model-free and model-based world model, we can use the environment in figure \ref{fig:bg:mb-rl} as a drop-in replacement of the environment to train the model-based controller.}
  \label{fig:problem:sys-env}
\end{figure}

To simplify the implementation of the environment, we used made extensive use of the work by Jia et al. \cite{jia2019taso} with the open source version of TASO. We provide a computation graph and the chosen transformation and location; TASO then applies the requested transformation and returns the newly transformed graph. Further, we use internal TASO functions that calculates estimates of the runtime on the hardware device which we use as our reward signal for training the agent. During our experiments we modified TASO to extract detailed runtime measurements to analyse the rewards using a range of different reward formulae---we provide more detail in section \ref{sec:prob:subsec:rwd}.

The scope of our work meant that there was no existing prior work that applied reinforcement learning to the task of optimising deep learning computation graphs. Thus, we required an environment in which an agent can act efficiently. Due to the nature of systems environments, the interactions with the real-world environment can be often slow, especially compared to those such as Arcade Learning Environment \cite{Bellemare_2013}. An aim of this work was to train a simulated environment, a ``world model'', that if accurate in relation to the real environment, we can train an agent far more efficiently than would be possible with the real-environment. In sections \ref{sec:rlopt:subsec:mb-agent} and \ref{sec:eval:subsec:mbagent} we will further explore world models and evaluate our implementation respectively.

\subsection{Computation Graphs}
The first step prior to optimising a deep learning graph is that we must load, or create on-demand, the model in a supported deep learning framework. In our project, we can support any model that is serialised into the ONNX \cite{bai2019onnx} format which is a open-source standard for defining the structure of deep learning models. Thereby, by extension, we can support any deep learning framework that supports ONNX serialisation such as TensorFlow \cite{tensorflow2015-whitepaper}, PyTorch \cite{pytorch} and MXNet \cite{chen2015mxnet}.

Next, we parse the ONNX graph internal representation by converting all operators into the equivalent TASO tensor representations such that we can modify the graph using the environment API as we described in section \ref{sec:prob:subsec:sysenv}. Although our environment does not support conversion of all operators defined in the ONNX specification \footnote{ONNX operator specification:~\url{https://github.com/onnx/onnx/blob/master/docs/Operators.md}}, the majority of the most common operators for our use case are supported; therefore we still maintain the semantic meaning and structure of the graph. Additionally, after performing optimisations of the graph, we can export the optimised graph directly to an ONNX format.

\subsection{State-Action space}
\label{sec:prob:subsec:sap}
In this project we modelled the state and action space in accordance with prior research, specifically we referenced work in a similar domain of system optimisation using reinforcement learning; Mirhoseini et al. \cite{mirhoseini2018hierarchical} used hierarchical RL with multiple actions to find the optimal device placement and Addanki et al. \cite{addanki2019placeto} that also aided in the design choice of input/output graph sizes.

Next, we require two values in order to update the environment. First, we need a select a transformation (which we refer to as an \texttt{xfer}) to apply to the graph. Secondly, the location at which to apply the transformation. As we need to select two actions that are dependent on each other to achieve a higher performance, it requires selecting the actions simultaneously.

However, this would require a model output of $N \times L$ values, where $N$ is the number of transformations, $L$ is the number of locations. Such an action space is too large to train a model to efficiently predict the correct action. Additionally, after choosing a transformation, we ideally mask the available locations as not all locations can be used to apply a transformation. Therefore, using the same trunk network, we first predict the transformation, apply the location mask for the selected transformation, then predict the location.

We define the action as 2-value tuple of (xfer\textunderscore id, location). There is a special case for the xfer\textunderscore id. When it equals N (the number of available transformations), we consider it the NO-OP action. Therefore, in this special case we do not modify the graph, rather we terminate the current episode and reset the environment to its initial state.

As explained in the previous section, we used an step-wise approach where at each iteration, we provide a 2-tuple of the transformation and location, to apply in the current state. The updated state from the environment is a 4-tuple consisting of \texttt{(graph\textunderscore tuple, xfer\textunderscore tuples, location\textunderscore masks, xfer\textunderscore mask)}.

\texttt{xfer\textunderscore mask} refers to a binary mask that indicates the valid and invalid transformations that can be applied to the current computation graph as not every transformation can be applied to every graph. If the current graph has only four possible transformations that can be applied, all other transformations considered to be invalid. Thus, we return a boolean location mask where only valid transformations are set to 1, or \texttt{true}. This can be used to zero-out the model logits of invalid transformations (and thereby actions also) to make ensure the agent always selects a valid transformation from the set.

Similarly, for each transformation selected by the agent, there are a number of valid locations where this transformation can be applied. We set a hardcoded, albeit configurable, limit the number of locations to 200 in this work. If the current graph has fewer than 200 possible locations for any given transformation, the remaining are considered invalid. Therefore, we again return a boolean location mask, which is named \texttt{location\textunderscore masks} in the 4-tuple defined above, which can be used to zero out the model logits that which the locations are invalid. 

% We use the location masks for every transformation, therefore, we have a $N \times L$ tensor, where $N$ is the number of transformations, and $L$ is the maximum number of valid locations.

\subsection{Reward function}
\label{sec:prob:subsec:rwd}
The design of a reinforcement learning agent consists of three key elements, the agent, environment and reward function. Most importantly, we require a reward function that captures dynamics of the environment in such a way that we can directly indicate to the agent if we consider the action to be ``good'' or ``bad''. For example, we wish to prevent the agent from performing actions that would be invalid in the environment, therefore, using the reward signal we provide a large negative reward to disincentivize the agent from replicating the behaviour. Conversely, we need to provide a positive reward, dependent on a chosen action and its impact on the agent performance.

Selecting optimal actions can be challenging in any deep reinforcement learning system, especially those with either long-term action dependencies or a large number of possible actions in any given state. Importantly, in our environment, the selection of a poor action be impactful on both subsequent action space and the resulting reward generated by the environment. Therefore, we used multiple reward functions to investigate the resulting performance of the agent. First, we used a simple reward function that is commonly used in sequential RL applications:

$$
r_t =
\begin{cases}
  RT_{t-1} - RT_t, & \text{if valid action}\\
  \text{-}100,            & \text{otherwise}
\end{cases}
$$

Using the reward function defined above, we use the previous estimated runtime, $RT_{t-1}$ of the computation graph and the estimated runtime of the current graph, $RT_t$, to determine the step-wise, incremental change in graph runtime as the reward. This simple, yet powerful function has the benefit of a very low overhead as we only need to store the last runtime. Furthermore, as our primary goal is to reduce the execution time of the graphs, rather than for example the system memory, it captures our desired metric for which we wish to optimise.

Secondly, we instrumented TASO to extract detailed metrics in an attempt to engineer a more complex reward function; we used the runtime, FLOPS, memory accesses and kernel launches to perform experiments to determine if using a combination of the metrics could yield a higher performance RL agent. We defined modified reward function as shown below; where $RT$ is the graph runtime, $M$ is the memory accesses, $\alpha$ and $\beta$ are two hyperparameters for weighting the runtime and memory accesses respectively.

$$
r_t =
\begin{cases}
  r_t = \alpha(RT_{t-1} - RT_t) + \beta(M_{t-1} - M_t), & \text{if valid action}\\
  \text{-}100,            & \text{otherwise}
\end{cases}
$$

We provide further discussion and motivation for our chosen reward functions in section \ref{sec:eval:subsec:mf:subsubsec:rwd-func} as well as an analysis of the detailed runtime metrics and the impact on improving graph runtime.

Finally, we note that TASO used a simple method to estimate the runtime of tensor operators that is executed using low-level CUDA APIs and the runtime is averaged over $N$ forward passes. However, this approach to runtime estimation is imperfect as there is a non-negligible variance of the runtime on real hardware and can lead to a poor estimation of the hardware impact. As such, we investigated the use of using real runtime measurements during training rather than a estimation of operator runtime. After performing experiments with a modified version of TASO which averages the real runtime over $N$ rounds, we found that it increases duration of each training step to such a degree that any possible performance improvements achieved using real hardware costs are not worth the trade-off.

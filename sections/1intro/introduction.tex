\chapter{Introduction}

Services we often take for granted on a day-to-day basis, such as search, social networks and language translators are composed of many complex systems, many of which are underpinned by machine learning (ML), specifically, deep neural networks (DNN). Over the past decade there has been a focus on developing frameworks for providing tools from which we can design, train and evaluate these networks.

A common internal representation for neural networks inside deep learning frameworks is that of a computation graph---a directed acyclic graph where nodes represent a specific computation and edges the paths where data is transferred. Frameworks such as TensorFlow \cite{tensorflow2015-whitepaper} and PyTorch \cite{pytorch} automatically apply optimisations in an effort to reduce computation resources during inference.

Currently, the optimisation performed in deep learning frameworks is performed using complex, manually defined heuristics. For example, TensorFlow \cite{tensorflow2015-whitepaper} uses 155 handwritten optimisations composed of 53,000 lines of C++. While such heuristics are applicable for current network architectures, the system requires consistent innovation to discover and design rules to apply optimisations which is guaranteed to either improve efficiency, or not regress. Eliminating the need for manual engineering work that is required to design and implement the heuristics for applying optimisations is a primary focus of this work.

Recent work, namely TASO by \citet{jia2019optimizing,jia2019optimizing} has shown it is possible replace the heuristics with a cost-based search for the optimal graph. However, such approaches may not fully explore the potential search space due to the lack of planning in a cost-based optimisation. To address this issue in this work we explore the use of reinforcement learning (RL); RL is an area of machine learning in which an agent learns to act optimally, given a state and a suitable reward function, through interactions with an environment. This work focuses on the use of RL for the task of optimising computation graphs---the internal representation of neural networks in ML frameworks.

Specifically, we focus on an approach called model-based reinforcement learning which aim to learn a model of the environment in which they act. In our work, they learn a model of the dynamics of the application of graph substitutions and its impact on the overall runtime of the model when executed on-device. Further, learning a model of the environment provides important benefits; for example, lookahead planning, cheap state prediction and faster training. This work examines the use of world-models for learning the environment as well as training a controller inside a world model that removes the need of an expensive, time-consuming computer system to apply our chosen graph optimisations.

This dissertations key contributions are:

\begin{itemize}
  \item Applies modern reinforcement learning approaches that eliminates the need for human engineered graph optimisations in machine learning frameworks.
  \item Provides a detailed discussion and analysis of our solution as well as comparison to the current state-of-the-art methods in published literature.
  \item Implemented a model-based RL agent (section \ref{sec:rlopt:subsec:mb-agent}), and environment (section \ref{sec:prob:subsec:sysenv}), for jointly choosing the optimal substitution and substitution location (section \ref{sec:prob:subsec:sap}).
  \item This work, to the best of our knowledge, is the first that has applied model-based reinforcement learning in optimising computation graphs to reduce hardware resource requirements.
\end{itemize}

The rest of the dissertation is structured as follows. Chapter 2 provides a background for computation graphs and the representation of deep learning models, reinforcement learning---both model-free and model-based---in the context of computer systems. Chapter 3 concretely introduces the optimisation problem and formulates the problem in the context of reinforcement learning. Furthermore, we also describe our approach for applying reinforcement learning to optimise the computation graphs as well as learning an accurate model of the environment. Chapter 4 covers the evaluation setup, our experiments and results for different methodologies. Finally, in chapter 5 we conclude the dissertation with a summary of our findings and discuss potential future work.
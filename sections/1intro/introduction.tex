\chapter{Introduction}

Services we often take for granted, such as web search, social networks and language translators, are composed of complex systems. Modern services have components that are underpinned by machine learning (ML) models, specifically, deep neural networks (DNN). Over the past decade there has been a focus on developing frameworks that provide tools using which we can design, train and evaluate these deep learning models.

A common internal representation for neural networks inside deep learning frameworks is that of a computation graph---a directed acyclic graph where nodes represent a specific computation and edges the paths where data is transferred. Frameworks such as TensorFlow \cite{tensorflow2015-whitepaper} and PyTorch \cite{pytorch} automatically apply optimisations in an effort to reduce computation resources during inference.

Currently, the optimisation in deep learning frameworks is performed using manually defined heuristics. For example, TensorFlow \cite{tensorflow2015-whitepaper} uses 155 handwritten optimisations composed of 53,000 lines of C++. While such heuristics are applicable for current architectures, network design is consistently evolving. Therefore, we require consistent innovation to discover and design rules that control the application of optimisations with guarantees that strictly improve efficiency. Eliminating the need for manual engineering work that is required to design and implement the heuristics for applying optimisations is a primary focus of this work.

Recent work, namely TASO by \citet{jia2019taso,jia2019optimizing} has shown it is possible replace the heuristics with a cost-based search. However, such approaches may not fully explore the potential search space due to the lack of forward planning in cost-based optimisation. As a step towards resolving the issue of poor exploration, this work explores the use of reinforcement learning (RL). RL is an area of machine learning in which an agent learns to act optimally, given a state and a suitable reward function, through interactions with an environment.

%This work focuses on the use of RL for the task of optimising computation graphs, the internal representation of neural networks in ML frameworks, in order to reduce on-device runtime.

In this work, we focus on the use of RL for the task of optimising deep learning graphs. Specifically, we focus on a model-based reinforcement learning which aims to learn a model of the environment in which they act. In our work, the network learns to model the dynamics of sub-graph transformations in a deep learning model as well as the impact on overall runtime of the model when executed on-device. Further, learning a model of the environment provides important benefits; for example, lookahead planning, low-cost state prediction and faster wall-clock training. We examine the use of world-models for learning the environment as well as training a controller inside a world model which removes the need of an expensive, time-consuming computer system to apply our chosen graph transformations.

This dissertations key contributions are:

\begin{itemize}
  \item Applies modern reinforcement learning approaches that eliminates the need for human engineered graph optimisations in machine learning frameworks. We show that our proposed method can improve runtime by up to 58\% compared to current deep learning frameworks and up to 10\% compared to the state-of-the-art.
  \item Provides a detailed discussion and analysis of our solution as well as comparison to the current state-of-the-art methods in published literature.
  \item Implemented a model-based RL agent (section \ref{sec:rlopt:subsec:mb-agent}), and environment (section \ref{sec:prob:subsec:sysenv}), for jointly choosing the optimal substitution and substitution location (section \ref{sec:prob:subsec:sap}).
  \item This work, to the best of our knowledge, is the first that has applied model-based reinforcement learning in optimising computation graphs to reduce hardware resource requirements.
\end{itemize}

The rest of the dissertation is structured as follows. Chapter 2 provides a background for computation graphs and the representation of deep learning models, reinforcement learning---both model-free and model-based---in the context of computer systems. Chapter 3 concretely introduces the optimisation problem and formulates the problem in the context of reinforcement learning. Furthermore, we also describe our approach for applying reinforcement learning to optimise the computation graphs as well as learning an accurate model of the environment. Chapter 4 covers the evaluation setup, our experiments and results for different methodologies. Finally, in chapter 5 we conclude the dissertation with a summary of our findings and discuss potential future work.
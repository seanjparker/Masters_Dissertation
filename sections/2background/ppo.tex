{\SetAlgoNoLine
\begin{algorithm}[H]
 Input: initial policy parameters $\theta_0$, clipping threshold $\epsilon$ \\
 \For{k = 0, 1, 2, \dots}{
  Collect set of partial trajectories $\mathcal{D}_k$ on policy $\pi_k = \pi(\theta_k)$ \\
  Estimate advantages $\hat{A}_t^{\pi_k}$ using GAE with the value function $V_{\phi_k}$ \\
  Compute policy update
  \[
  \theta_{k+1} = \argmax_{\theta} \mathcal{L}_{\theta_k}^{\text{CLIP}}(\theta)
  \]
  by taking $K$ steps of minibatch SDG (using Adam), where
  \[
  \mathcal{L}_{\theta_k}^{\text{CLIP}}(\theta) = \mathbb{E}_{\tau \sim \pi_k} \left[ \sum_{t=0}^T \left[ \min (r_t(\theta) \hat{A}_t^{\pi_k}, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t^{\pi_k}) \right] \right]
  \]
 Fit value function using using MSE loss using minibatch SDG
 \[
 \phi_{k+1} = \argmin_{\phi} \frac{1}{|\mathcal{D}_k|T}  \sum_{t=0}^T \left( V_\phi \left( s_t \right) - \hat{R}_t \right)^2
 \]
 }
 \caption{PPO with Clipped Objective}
 \label{algo1}
\end{algorithm}}
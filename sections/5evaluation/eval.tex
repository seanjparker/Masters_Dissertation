\chapter{Evaluation}

\section{Aims}

In this chapter, we look to assess aims we presented at the beginning of this work where we claimed to use reinforcement learning to perform automated optimisation of deep learning computation graphs. Thus, this evaluation seeks to answer the following questions:

\begin{enumerate}
  \item Are model-based reinforcement learning methods able to model the transition dynamics of the environment?
  \item Is the agent policies able to generalise to unseen states of the same graph to act in accordance to our performance objectives?
  \item Do they accurately model the reward estimation from the graph latent state?
  \item Are the agents trained in an imagined world model applicable to the real-world environment?
\end{enumerate}

\section{Experimental Setup}

All the experiments presented in this chapter, both training various agent models and testing, is performed using the codebase available in the GitLab repository for this project [todo: link]. The project was developed, and the experiments were performed using a single machine running Ubuntu Linux 18.04 with a 6-core Intel i7-10750H@2.6GHz, 16GB RAM and an NVIDIA GeForce RTX 2070.

We chose to use five real-world deep learning models to evaluate our project. InceptionV3 \cite{szegedy2015rethinking} is a common, high-accuracy model for image classification trained on the ImageNet dataset. ResNet-18 \& ResNet-50 \cite{he2015deep} are also deep convolutional networks, 18 and 50 layers deep respectively as well as SqueezeNet \cite{iandola2016squeezenet}, a shallower yet accurate model. BERT \cite{devlin2019bert} is a recent large transformer network that is used to improve Google search results \cite{nayak2019}. As these graph were also used in the evaluation of TASO \cite{jia2019taso}, we can show a direct comparison of the performance between the different approaches.

To interface with the internal representation of the computation graphs, as previously discussed, we used the open-sourced version of TASO \cite{jia2019taso} which we modified to extract detailed runtime information. Further, we implemented the reinforcement learning algorithms in TensorFlow 2 \cite{tensorflow2015-whitepaper} and utilised the \texttt{graph\textunderscore nets} package developed by Battaglia et al. \cite{battaglia2018relational} to process our input graphs which we described in chapter \ref{sec:prob:subsec:sysenv}. The PPO agent was implemented based upon the implementation provided by Schulman et al. \cite{schulman2017proximal}.

Finally, we would like to acknowledge the work by Kai Fricke and Michael Schaarschmidt who developed the initial Python interface with TASO, the algorithm for  converting the C++ TASO graph representation into a Cython object and performed experiments with model-free reinforcement learning agents \cite{xflowrl2019}.

\section{Experiments}

% Baselines (TASO, TensorFlow, TVM etc)

% Model-free experiments

% Model-based experiments
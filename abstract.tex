\newpage
{\Huge \bf Abstract}
\vspace{24pt} 

This project investigates the use of model-based reinforcement learning (RL) in the domain of computer systems, specifically, that of optimising deep learning models by using RL to choose the graph transformations which are applied the networks graph representation. Reducing the hardware resource requirements of deep learning models is a open, active research question; current work has focused on the design optimal heuristic rules. In this work, we investigated the use of RL agents that can learn to perform optimal transformations, without the need of expert human heuristics to achieve a high level of performance. Recent work has aimed to apply reinforcement learning to computer systems with some success, especially using model-free RL techniques. However, model-based methods have seen an increased focus of research; it can be used to learn a model of the environment that can be leveraged to train an agent inside the learned world-model, thereby increasing sample efficiency compared to model-free RL. Furthermore, when using a world model as the environment, batch rollouts can occur safely in parallel and, especially in systems environments, it overcomes the possible latency impact of stepping a system environment that can take orders of magnitude longer to perform an action compared to a simple emulators for video games. This dissertation examines both the prior work for optimising deep learning models and the applicability of reinforcement learning to the problem. We show that by using model-based RL, we can reduce the runtime of deep learning models by up to $58\%$ compared to current deep learning frameworks and up to 7\% compared to the state of the art approaches.

\vspace*{\fill}

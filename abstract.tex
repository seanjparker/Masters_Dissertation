\newpage
{{\textbf{Model-based Reinforcement Learning in Computer Systems}}\\

{\Huge \bf Abstract}
\vspace{24pt} 

This project investigates the use of model-based reinforcement learning (RL) in the domain of computer systems, specifically, that of optimising deep learning models by using RL to choose the graph transformations which are to the graph representation of deep learning models. Reducing the hardware resource requirements of deep learning models is a open, active research question; where current work has focused on the design optimal heuristic rules. We explored the use of RL agents that can learn to perform optimal transformations, without the need of expertly designed heuristics to achieve a high level of performance. Recent work has aimed to apply reinforcement learning to computer systems with some success, especially using model-free RL techniques. However, model-based methods have seen an increased focus of research; as they can be used to learn the transition dynamics of the environment which can then be to leveraged train an agent using the learnt environment representation; thereby increasing sample efficiency compared to model-free RL. Furthermore, when using a world model as the environment, batch rollouts can occur safely in parallel and, especially in systems environments, it overcomes the possible latency impact of stepping a system environment that can take orders of magnitude longer to perform an action compared to simple emulators for video games. This dissertation examines both the prior work for optimising deep learning models and the applicability of reinforcement learning to the problem. We show that by using model-based RL, we can reduce the runtime of deep learning models by up to $58\%$ compared to current deep learning frameworks and up to $10\%$ compared to state of the art approaches.

\vspace*{\fill}
